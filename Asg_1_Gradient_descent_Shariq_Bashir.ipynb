{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent:\n",
    "\n",
    "Gradient descent is an optimization algorithm commonly used in machine learning and neural networks to minimize errors between predicted and actual results. It works by iteratively adjusting the parameters of a model to reduce the cost function, which measures the difference between the predicted and actual values.\n",
    "\n",
    "Here's a simplified explanation of how it works:\n",
    "\n",
    "1. **Initialization**: Start with an initial set of parameters (weights and biases).\n",
    "2. **Compute Gradient**: Calculate the gradient (partial derivatives) of the cost function with respect to each parameter. This gradient indicates the direction and rate of the steepest increase in the cost function.\n",
    "3. **Update Parameters**: Adjust the parameters in the opposite direction of the gradient to reduce the cost function. The size of these adjustments is determined by the learning rate.\n",
    "4. **Iterate**: Repeat the process until the cost function converges to a minimum value or a predefined number of iterations is reached.\n",
    "\n",
    "Gradient descent can be visualized as rolling a ball down a hill, where the goal is to reach the lowest point (minimum cost). There are different variants of gradient descent, such as stochastic gradient descent (SGD) and mini-batch gradient descent, which differ in how they update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Gradient Descent:\n",
    "\n",
    "There are three main types of gradient descent, each differing in how they process the training data:\n",
    "\n",
    "1. **Batch Gradient Descent**:\n",
    "   - **Description**: This method calculates the gradient of the cost function with respect to the parameters for the entire training dataset. It updates the parameters only once per epoch.\n",
    "   - **Advantages**: It provides a stable convergence and a smooth error gradient.\n",
    "   - **Disadvantages**: It can be computationally expensive and slow, especially for large datasets, as it requires loading the entire dataset into memory.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**:\n",
    "   - **Description**: This method updates the parameters for each training example one at a time. It calculates the gradient and updates the parameters for each individual data point.\n",
    "   - **Advantages**: It is faster and can handle large datasets as it processes one sample at a time. It can also escape local minima due to its noisy updates.\n",
    "   - **Disadvantages**: The error gradient can be noisy, leading to fluctuations in the cost function.\n",
    "\n",
    "3. **Mini-Batch Gradient Descent**:\n",
    "   - **Description**: This method is a compromise between batch and stochastic gradient descent. It divides the training dataset into small batches and updates the parameters for each batch.\n",
    "   - **Advantages**: It combines the benefits of both batch and stochastic gradient descent. It provides a more stable convergence than SGD and is more efficient than batch gradient descent.\n",
    "   - **Disadvantages**: The choice of batch size can affect the performance and convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set\n",
    "A **validation set** is a portion of the dataset set aside during the training process to evaluate the performance of a machine learning model. It is not used for training the model but rather for tuning the model's hyperparameters and preventing overfitting. By assessing the model on the validation set, you can get an idea of how well it will perform on unseen data.\n",
    "\n",
    "# Validation Loss\n",
    "**Validation loss** is a metric that measures the error of the model on the validation set. It is similar to training loss, which measures the error on the training set, but it provides an indication of how well the model generalizes to new data. Validation loss is calculated after each epoch (a complete pass through the training data) and helps in determining whether the model needs further tuning or adjustments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
